---
title: "Machine Learning"
author: "KHemzacek"
date: "September 6, 2017"
output: html_document
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library("dplyr")
library("tidyr")
library("ggplot2")
library("knitr")
library("rpart")
library("rpart.plot")
library("caret")
library("e1071")

parkinsons <- read.csv("parkinsons_clean.csv")
parkAvgd <- read.csv("parkinsons_averaged.csv")
patient_data <- read.csv("parkinsons_patient_data.csv")
corCoeff <- readRDS("corCoeffList.rds")
```

corCoeff was created in exploratory data analysis section, saved, and loaded into this document. It contains 100 full correlation matrices describing the relationships between all variables of interest.

# Investigate Correlation Coefficients
## Correlation with motor UPDRS

```{r updrs_correlation}
N = 100
updrs_correlation <- corCoeff[[1]][5, ] # initiate matrix
for(i in 2:N){
  updrs_correlation <- rbind(updrs_correlation, corCoeff[[i]][5, ]) # select UPDRS coefficients from each correlation matrix; aggregate into matrix
}

# change to dataframe
row.names(updrs_correlation) <- NULL
updrs_correlation <- as.data.frame(updrs_correlation)
updrs_correlation$test_day <- NULL
updrs_correlation$subject_num <- NULL
updrs_correlation$motor_UPDRS <- NULL
updrs_correlation$total_UPDRS <- NULL

# transform to best shape for plotting
plotRdata <- gather(updrs_correlation, "Feature", "Correlation", 1:66)

# box-whisker plot of correlations with motor_UPDRS
plot(as.factor(plotRdata$Feature), plotRdata$Correlation)
lines(x = 0:67, y = rep(0, times = 68))
lines(x = 0:67, y = rep(0.2, times = 68))
lines(x = 0:67, y = rep(-0.2, times = 68))
```

Criteria:
* median greater than +/-0.2 (or close to it)
* relatively small spread
* entire range doesn't cross 0

Best Candidates:
* age (pos)1
* loud_HNR_median (neg)5
* loud_PPE_median (pos)19
* loud_Shimmer_APQ11_median(pos)23?
* normal_HNR_median (neg)37
* normal_PPE_median (pos)51
* normal_RPDE_median (pos)53

Good Candidates (large median (pos/neg); low IQR):
* loud_NHR_median (pos)17*
* loud_RPDE_median (pos)21
* loud_Shimmer_APQ5_median (pos)27
* loud_Shimmer_dB_median (pos)29
* loud_Shimmer_median (pos)33
* normal_DFA_median (neg)35*
* normal_Jitter_Percent_median (pos)43
* normal_Jitter_PPQ5_median (pos)45
* normal_NHR_median (pos)49
* normal_Shimmer_APQ3_median (pos)57
* normal_Shimmer_DDA_median (pos)63
* time_of_day (neg)66*


## Correlation between features with high correlation with UPDRS

```{r intercorrelation}
bestCandidates <- c("age", "loud_HNR_median", "loud_PPE_median", "loud_Shimmer_APQ11_median", "normal_HNR_median", "normal_PPE_median", "normal_RPDE_median")

# create list of only correlations between best candidate features
bestCandCoeff <- list(NULL) # initiate list
for(i in 1:length(corCoeff)){
   bestCandCoeff[[i]] <- subset(corCoeff[[i]], rownames(corCoeff[[i]]) %in% bestCandidates, colnames(corCoeff[[i]]) %in% bestCandidates) # select and store limited correlation matrices
}

# create list, each item containing all cor coeffs relating to one feature
intercorrelation <- list(NULL) # initiate list
for(i in 1:length(bestCandidates)){
  # delete previous matrix and initiate new matrix
  candidate_correlation <- NULL
  candidate_correlation <- subset(bestCandCoeff[[1]], rownames(bestCandCoeff[[1]]) == bestCandidates[i])
  
  # select row of cor coeffs from each correlation matrix
  for(j in 2:length(bestCandCoeff)){
    candidate_correlation <- rbind(candidate_correlation, subset(bestCandCoeff[[j]], rownames(bestCandCoeff[[j]]) == bestCandidates[i]))
  }
  
  # convert matrix to dataframe
  rownames(candidate_correlation) <- NULL
  candidate_correlation <- as.data.frame(candidate_correlation)
  intercorrelation[[i]] <- candidate_correlation # save dataframe to list
}
names(intercorrelation) <- bestCandidates

for(i in 1:length(intercorrelation)){
  plotData <- NULL
  # transform data into best shape for plotting
  plotData <- gather(intercorrelation[[i]], "Feature", "Correlation", 1:length(bestCandidates))
  # delete coefficients describing feature's correlation with itself
  plotData <- subset(plotData, Feature != bestCandidates[i])
  # box plot of correlation coefficients, relating to one feature candidate
  plot(as.factor(plotData$Feature), plotData$Correlation, main = bestCandidates[i])
  lines(x = 0:length(bestCandidates), y = rep(0, times = length(bestCandidates) + 1))
  lines(x = 0:length(bestCandidates), y = rep(0.2, times = length(bestCandidates) + 1))
  lines(x = 0:length(bestCandidates), y = rep(-0.2, times = length(bestCandidates) + 1))
}

```

```{r all_candidates}

allCandidates <- c("age", "loud_HNR_median", "loud_NHR_median", "loud_PPE_median", "loud_RPDE_median", "loud_Shimmer_APQ11_median", "loud_Shimmer_APQ5_median", "loud_Shimmer_dB_median", "loud_Shimmer_median", "normal_DFA_median", "normal_HNR_median", "normal_Jitter_Percent_median", "normal_Jitter_PPQ5_median", "normal_NHR_median", "normal_PPE_median", "normal_RPDE_median", "normal_Shimmer_APQ3_median", "normal_Shimmer_DDA_median", "time_of_day")

# create list of only correlations between candidate features
allCandCoeff <- list(NULL) # initiate list
for(i in 1:length(corCoeff)){
   allCandCoeff[[i]] <- subset(corCoeff[[i]], rownames(corCoeff[[i]]) %in% allCandidates, colnames(corCoeff[[i]]) %in% allCandidates)
}

# create list, each item containing all cor coeffs relating to one feature
intercorrelation <- list(NULL) # initiate list
for(i in 1:length(allCandidates)){
  # delete previous matrix and initiate new matrix
  candidate_correlation <- NULL
  candidate_correlation <- subset(allCandCoeff[[1]], rownames(allCandCoeff[[1]]) == allCandidates[i])
  
  # select row of cor coeffs from each correlation matrix
  for(j in 2:length(allCandCoeff)){
    candidate_correlation <- rbind(candidate_correlation, subset(allCandCoeff[[j]], rownames(allCandCoeff[[j]]) == allCandidates[i]))
  }
  
  # convert matrix to dataframe
  rownames(candidate_correlation) <- NULL
  candidate_correlation <- as.data.frame(candidate_correlation)
  intercorrelation[[i]] <- candidate_correlation # save dataframe to list
}
names(intercorrelation) <- allCandidates

for(i in 1:length(intercorrelation)){
  plotData <- NULL
  # transform data into best shape for plotting
  plotData <- gather(intercorrelation[[i]], "Feature", "Correlation", 1:length(allCandidates))
  # delete coefficients describing feature's correlation with itself
  plotData <- subset(plotData, Feature != allCandidates[i])
  # box plot of correlation coefficients, relating to one feature candidate
  plot(as.factor(plotData$Feature), plotData$Correlation, main = allCandidates[i])
  lines(x = 0:length(allCandidates), y = rep(0, times = length(allCandidates) + 1))
  lines(x = 0:length(allCandidates), y = rep(0.2, times = length(allCandidates) + 1))
  lines(x = 0:length(allCandidates), y = rep(-0.2, times = length(allCandidates) + 1))
}

```

sets:
age, 1 of (loud_NHR_median, normal_Jitter_Percent_median, normal_Jitter_PPQ5_median, normal_NHR_median)
loud_HNR_median, age, normal_DFA_median, time_of_day

* age has a fairly low correlation with everything
* normal_DFA_median has a fairly low correlation with everything
* time_of_day has a fairly low correlation with everything
* age, normal_DFA_median, and time_of_day have higher correlation with each other than with many other variables (but still low correlation relative to intercorrelation among other variables).
* sex could be added to any of these sets

# Split Test/Train Sets

```{r split_test_train}
groupwise_split <- function(X, obsRatio = 2/3, groupRatio = 1, seed = NULL) {
  set.seed(seed)
  groups <- unique(X)  # vector of unique group names
  index <- 1:length(X)  # create indices
  table <- cbind(index, X)  # bind indices and group labels
  
  obsInd <- list(NULL)  # initiate list
  for (i in 1:length(groups)){
    group_i <- subset(table, X == groups[i])[ ,1]  # select all indices of one group
    # randomly select % of indices of group i
    obsInd[[i]] <- sample(group_i, round(obsRatio * length(group_i)))
  }
  obsLogical <- index %in% unlist(obsInd)  # create logical vector where selected indices = TRUE
  
  # randomly select % of groups; create logical vector with these groups=TRUE
  groupLogical <- X %in% sample(groups, round(groupRatio * length(groups)))
  
  split <- ((obsLogical == TRUE) & (groupLogical == TRUE))
  return(split)
}

split <- groupwise_split(parkAvgd$subject_num, .8, 38/42, seed = 1)

train <- subset(parkAvgd, split == TRUE)
test <- subset(parkAvgd, split == FALSE)

```


# Function to select sets of independent observations
Set of independent obaservations = one random observation from each person

```{r independent_observations}
independent_selector <- function(matrix, X, N = 1) {
  groups <- unique(X)  # vector of unique group names
  index <- 1:length(X)  # create indices
  table <- cbind(index, X)  # bind indices and group labels
  
  obsInd <- list(NULL)  # initiate list
  for (i in 1:length(groups)){
    group_i <- subset(table, X == groups[i])[ ,1]  # select all indices of one group
    # randomly select N indices of group i
    obsInd[[i]] <- sample(group_i, size = N)
  }
  obsLogical <- index %in% unlist(obsInd)  # create logical vector where selected indices = TRUE

  selection <- subset(matrix, obsLogical == TRUE)
  return(selection)
}

```


# Linear Regression
## Create Standard Linear Regression Models

```{r standard_linear_regression}
# 2 variable model
# age most highly correlated with motor_UPDRS; normal_HNR_median lowest correlation with age of the top 7
mod <- lm(motor_UPDRS ~ age + normal_HNR_median, data = train)
# R2 = 0.08353

# 3 variable model
# add sex to the above model
mod <- lm(motor_UPDRS ~ age + normal_HNR_median + sex, data = train)
# R2 = 0.1051
# adding sex variable seemed to help a lot

# 7 variable model
# all top 7
mod <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median, data = train)
# R2 = 0.1119



# 8 variable model
# add sex to top 7 model
mod <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median + sex, data = train)
# R2 = 0.1236 / 0.1137
# age***, sex**

# remove loud_Shimmer_AQP11_median
mod <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median + sex, data = train)
# R2 = 0.1236 / 0.1149 (didn't change / went up)
# age***, sex**

# remove normal_HNR_median
mod <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + normal_PPE_median + normal_RPDE_median + sex, data = train)
# R2 = 0.1235 / 0.1161 (didn't change much / went up)
# age***, sex**, normal_PPE_median*, loud_PPE_median.

# remove loud_HNR_median
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + normal_PPE_median + normal_RPDE_median + sex, data = train)
# R2 = 0.1208 / 0.1146
# age***, loud_PPE_median***, sex**, normal_PPE_median*, normal_RPDE_median*

# remove normal_PPE_median
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + normal_RPDE_median + sex, data = train)
# R2 = 0.1152 /  0.1102
# age***, loud_PPE_median**, sex**

# add normal_DFA_median
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + normal_RPDE_median + sex + normal_DFA_median, data = train)
# R2 = 0.1522 / 0.1462
# Intercept***, age***, loud_PPE_median***, sex***, normal_DFA_median***

# remove normal_RPDE_median
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median, data = train)
# R2 = 0.1496 / 0.1448
# Intercept***, age***, loud_PPE_median***, sex***, normal_DFA_median***
# ALL coefficients highly significant

# BEST MODEL
# add time_of_day
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = train)
# R2 = 0.1622 / 0.1562
# Intercept***, age***, loud_PPE_median***, sex***, normal_DFA_median***, time_of_day**

# add normal_NHR_median
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day + normal_NHR_median, data = train)
# R2 = 0.1628 / 0.1557
# Intercept***, age***, loud_PPE_median***, sex**, normal_DFA_median***, time_of_day**
# R2 hardly went up; adjusted R2 went down; normal_NHR_median not signif

# remove normal_NHR median, add normal_PPE_median
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day + normal_PPE_median, data = train)
# R2 = 0.1622 / 0.1551
# Intercept***, age***, loud_PPE_median***, sex***, normal_DFA_median***, time_of_day**
# no change in R2 (no change from before normal_NHR_median); adjusted R2 went down; normal_PPE_median not signif



# Interactions
# add time_of_day interactions
mod <- lm(motor_UPDRS ~ age + time_of_day*loud_PPE_median + sex + time_of_day*normal_DFA_median , data = train)
# R2 = 0.1986 / 0.1906
# Intercept*, age***, time_of_day***, loud_PPE_median*, sex***, normal_DFA_median**, time_of_day:loud_PPE_median, time_of_day:normal_DFA_median***

# remove time_of_day:loud_PPE_median interaction
mod <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + time_of_day*normal_DFA_median , data = train)
# R2 = 0.1986 / 0.1918 (no change / went up)
# Intercept*, age***, loud_PPE_median***, sex***, time_of_day***, normal_DFA_median**, time_of_day:normal_DFA_median***

# OTHER BEST
# add time_of_day/age/sex interaction
mod <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = train)
# R2 = 0.2444 / 0.2348
# everything highly significant

# add this interaction to both signal characteristics
mod <- lm(motor_UPDRS ~ age*sex*time_of_day*loud_PPE_median + age*sex*time_of_day*normal_DFA_median, data = train)
# R2 = 0.3509 / 0.3293
# nothing is highly significant

# add this interaction to one signal characteristic at a time
mod <- lm(motor_UPDRS ~ loud_PPE_median + age*sex*time_of_day*normal_DFA_median, data = train)
# R2 = 0.34 / 0.3249
# not much is highly significant, loud_PPE_median***
mod <- lm(motor_UPDRS ~ age*sex*time_of_day*loud_PPE_median + normal_DFA_median, data = train)
# R2 = 0.2708 / 0.2541
# not much is highly significant, normal_DFA_median***

# add age/sex interaction
mod <- lm(motor_UPDRS ~ age*sex + loud_PPE_median + normal_DFA_median + time_of_day, data = train)
# R2 = 0.1637 / 0.1566
# R2 and adjusted R2 not much up from best model w/out interactions; age:sexmale not significant

# add age interactions
mod <- lm(motor_UPDRS ~ age*loud_PPE_median + sex + age*normal_DFA_median + time_of_day, data = train)
# R2 = 0.1768 / 0.1686
# age:loud_PPE_median not significant
# remove age:loud_PPE_median interaction
mod <- lm(motor_UPDRS ~ loud_PPE_median + sex + age*normal_DFA_median + time_of_day, data = train)
# R2 = 0.1765 / 0.1695
# removing interaction - R2 similar, adjusted R2 up; age**, time_of_day., all others***

# age and time_of_day with normal_DFA_median
mod <- lm(motor_UPDRS ~ loud_PPE_median + sex + age*time_of_day*normal_DFA_median, data = train)
# R2 = 0.2604 / 0.2509
# loud_PPE_median***, sexmale***, all others not very significant

# add sex interactions
mod <- lm(motor_UPDRS ~ age + sex*loud_PPE_median + sex*normal_DFA_median + time_of_day, data = train)
# R2 = 0.1724 / 0.1642
# (Intercept)***, age***, time_of_day***, normal_DFA_median**, sexmale:loud_PPE_median**, others not significant

# sex/PPE, age/time/DFA
mod <- lm(motor_UPDRS ~ loud_PPE_median*sex + age*time_of_day*normal_DFA_median, data = train)
# R2 = 0.2618 / 0.2513
# almost nothing significant

# PPE/DFA interaction
mod <- lm(motor_UPDRS ~ age + sex + loud_PPE_median*normal_DFA_median + time_of_day, data = train)
# R2 = 0.1858 / 0.1789
# sexmale**, everything else highly significant

# BEST MODEL
# age/sex/time, PPE/DFA
mod <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = train)
# R2 = 0.2653 / 0.2548
# everything highly significant except intercept, which isn't

# age/sex/time/PPE/DFA
mod <- lm(motor_UPDRS ~ age*sex*time_of_day*loud_PPE_median*normal_DFA_median, data = train)
# R2 = 0.3622 / 0.3332
# nothing is significant
```


## Create Averaged Linear Regression Models

```{r independent_linreg}
# creating model with many subsets of indep vars
N = 100


# 2 variable model
model <- lm(motor_UPDRS ~ age + normal_HNR_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + normal_HNR_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
modVars <- select(train, motor_UPDRS, age, normal_HNR_median)
modVars <- mutate(modVars, prediction = modFunction$`(Intercept)` + age * modFunction$age + normal_HNR_median * modFunction$normal_HNR_median,  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.08176474, 0.07713716, 0.08309732, 0.08172012, 0.07713845


# 3 variable model
model <- lm(motor_UPDRS ~ age + normal_HNR_median + sex, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + normal_HNR_median + sex, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
modVars <- select(train, motor_UPDRS, age, normal_HNR_median, sex)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = modFunction$`(Intercept)` + age * modFunction$age + normal_HNR_median * modFunction$normal_HNR_median + sex * modFunction$sexmale,  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 #  0.103931, 0.1037599, 0.1038772, 0.1039132, 0.104196


# 7 variable model
model <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
modVars <- select(train, motor_UPDRS, age, loud_HNR_median, loud_PPE_median, loud_Shimmer_APQ11_median, normal_HNR_median, normal_PPE_median, normal_RPDE_median)
modVars <- mutate(modVars, prediction = modFunction[1, 1] + age * modFunction[1, 2] + loud_HNR_median * modFunction[1, 3] + loud_PPE_median * modFunction[1, 4] + loud_Shimmer_APQ11_median * modFunction[1, 5] + normal_HNR_median * modFunction[1, 6] + normal_PPE_median * modFunction[1, 7] + normal_RPDE_median * modFunction[1, 8],  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.08831432, -0.09374269, 0.07317749, 0.07200996, 0.08235688


# 8 variable model
model <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median + sex, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median + sex, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
modVars <- select(train, motor_UPDRS, age, loud_HNR_median, loud_PPE_median, loud_Shimmer_APQ11_median, normal_HNR_median, normal_PPE_median, normal_RPDE_median, sex)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = modFunction[1, 1] + age * modFunction[1, 2] + loud_HNR_median * modFunction[1, 3] + loud_PPE_median * modFunction[1, 4] + loud_Shimmer_APQ11_median * modFunction[1, 5] + normal_HNR_median * modFunction[1, 6] + normal_PPE_median * modFunction[1, 7] + normal_RPDE_median * modFunction[1, 8] + sex * modFunction[1, 9],  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.01576301, 0.09616189, -0.09369992, 0.1208045, 0.03753604


# Best model
model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
modVars <- select(train, motor_UPDRS, age, loud_PPE_median, sex, normal_DFA_median, time_of_day)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = modFunction[1, 1] + age * modFunction[1, 2] + loud_PPE_median * modFunction[1, 3] + sex * modFunction[1, 4] + normal_DFA_median * modFunction[1, 5] + time_of_day * modFunction[1, 6],  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1583306, 0.1612593, 0.1430441, 0.1533685, 0.1616463, 0.1546493, 0.1556214, 0.161885, 0.1418788, 0.1605511
# 0.1552234 +/- 0.007381991


# Interactions model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction[1, 1]) + (age * modFunction[1, 2]) + (sex * modFunction[1, 3]) + (time_of_day * modFunction[1, 4]) + (loud_PPE_median * modFunction[1, 5]) + (normal_DFA_median * modFunction[1, 6]) + (age * sex * modFunction[1, 7]) + (age* time_of_day * modFunction[1, 8]) + (sex * time_of_day * modFunction[1, 9]) + (loud_PPE_median * normal_DFA_median * modFunction[1, 10]) + (age * sex * time_of_day * modFunction[1, 11]),  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.2114609, 0.1635765, 0.2251201, -0.662214, -1.317873, -0.1947478, 0.2514686, 0.06678401, -0.02596546, 0.2505157
# -0.1031874 +/- 0.5120816
# highly unstable it seems... R2 ranges very good to very bad


# Other Interactions Model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction[1, 1]) + (age * modFunction[1, 2]) + (sex * modFunction[1, 3]) + (time_of_day * modFunction[1, 4]) + (loud_PPE_median * modFunction[1, 5]) + (normal_DFA_median * modFunction[1, 6]) + (age * sex * modFunction[1, 7]) + (age* time_of_day * modFunction[1, 8]) + (sex * time_of_day * modFunction[1, 9]) + (age * sex * time_of_day * modFunction[1, 10]),  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.08608255, 0.1519965, -0.0779731, 0.2324456, -0.2634722, 0.1598314, 0.1977368, 0.1277001, 0.2112764, 0.09585323
# 0.09214773 +/- 0.1527192
# less unstable than last interaction model, but still pretty unstable
```


# Regression Tree Models
## Standard Regression Tree Model (CART)

```{r basic_rpart}

# 2 variable model
mod <- rpart(motor_UPDRS ~ age + normal_HNR_median, data = train)
prp(mod)
# most splits rely on age; many lines predict value based purely on age
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.6890619


# 3 variable model
# add sex to the above model
mod <- rpart(motor_UPDRS ~ age + normal_HNR_median + sex, data = train)
prp(mod)
# age still defines a lot of splits; lines with only age or only age and sex
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.8317892


# 7 variable model
mod <- rpart(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median, data = train)
prp(mod)
# much more variety in variables used - better
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.688511

# 7 variables with minbucket
mod <- rpart(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median, data = train, control = rpart.control(minbucket = 25))
prp(mod)
# simpler tree, good variety of variables
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.4297927


# 8 variable model
mod <- rpart(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median + sex, data = train)
prp(mod)
# good variety of variables, but a couple lines predicted only from age
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.7219781

# 8 variables with minbucket
mod <- rpart(motor_UPDRS ~ age + loud_HNR_median + loud_PPE_median + loud_Shimmer_APQ11_median + normal_HNR_median + normal_PPE_median + normal_RPDE_median + sex, data = train, control = rpart.control(minbucket = 25))
prp(mod)
# simpler tree, one line based on age and sex only
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.4228446


# Best model
mod <- rpart(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = train)
prp(mod)
# hardly any decisions based on signal characteristics - not ok
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.8283099

# Best model with minbucket
mod <- rpart(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = train, control = rpart.control(minbucket = 25))
prp(mod)
# simpler tree, hardly any decisions based on signal characteristics
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.5895741


# Category Variables
mod <- rpart(motor_UPDRS ~ age + sex + normal_RPDE_median + loud_Shimmer_APQ11_median + normal_Jitter_Percent_median + loud_HNR_median + loud_PPE_median + normal_DFA_median, data = train)
prp(mod)
# decent variety, a few lines based only on age
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.7782156

# Category variables with minbucket
mod <- rpart(motor_UPDRS ~ age + sex + normal_RPDE_median + loud_Shimmer_APQ11_median + normal_Jitter_Percent_median + loud_HNR_median + loud_PPE_median + normal_DFA_median, data = train, control = rpart.control(minbucket = 25))
prp(mod)
# simpler tree, all predictions based on at least one signal characteristic
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.5453855


# Category Variables, -age
mod <- rpart(motor_UPDRS ~ sex + normal_RPDE_median + loud_Shimmer_APQ11_median + normal_Jitter_Percent_median + loud_HNR_median + loud_PPE_median + normal_DFA_median, data = train)
prp(mod)
# decent variety, a few lines based only on age
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.3943781

# Category variables, -age with minbucket
mod <- rpart(motor_UPDRS ~ sex + normal_RPDE_median + loud_Shimmer_APQ11_median + normal_Jitter_Percent_median + loud_HNR_median + loud_PPE_median + normal_DFA_median, data = train, control = rpart.control(minbucket = 25))
prp(mod)
# simpler tree, all predictions based on at least one signal characteristic
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.3057981


predVars <- select(train, -subject_num, -test_day, -contains("count"), -total_UPDRS)

# All variables
mod <- rpart(motor_UPDRS ~ ., data = predVars)
prp(mod)
# several lines based only on "context" variables, but otherwise decent
# age(6), time_of_day(5), 
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.7327982

# All variables with minbucket
mod <- rpart(motor_UPDRS ~ ., data = predVars, control = rpart.control(minbucket = 25))
prp(mod)
# one line based only on "context" variables, but otherwise decent
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.5694667


# use k-fold cross-validation to determine appropriate cp
# define cross-validation parameters
numFolds <- trainControl(method = "cv", number = 10)
cpGrid <- expand.grid(.cp = seq(0.01,0.5,0.01))
# perform cross validation
CV <- train(motor_UPDRS ~ ., data = predVars, method = "rpart", trControl = numFolds, tuneGrid = cpGrid, na.action = na.omit)
plot(CV$results$cp, CV$results$RMSE)
plot(CV$results$cp, CV$results$Rsquared)
# over this range of cp values, RMSE only gets bigger, levels off after cp=~0.13 at RMSE=~7.7. The optimal value may be smaller than 0.01

# retest with lower range of cp values
# define parameters
numFolds <- trainControl(method = "cv", number = 10)
cpGrid <- expand.grid(.cp = seq(0.001,0.03,0.001))
# perform cross validation
CV <- train(motor_UPDRS ~ ., data = predVars, method = "rpart", trControl = numFolds, tuneGrid = cpGrid, na.action = na.omit)
plot(CV$results$cp, CV$results$RMSE)
plot(CV$results$cp, CV$results$Rsquared)
# more like expected shape
# 0.002 - 0.007

# define parameters
numFolds <- trainControl(method = "cv", number = 10)
cpGrid <- expand.grid(.cp = seq(0.0001,0.02,0.0001))
# perform cross validation
CV <- train(motor_UPDRS ~ ., data = predVars, method = "rpart", trControl = numFolds, tuneGrid = cpGrid, na.action = na.omit)
plot(CV$results$cp, CV$results$RMSE)
plot(CV$results$cp, CV$results$Rsquared)
# more like expected shape
# 0.003 - 0.007

# check cps
mod <- rpart(motor_UPDRS ~ ., data = predVars, control = rpart.control(cp = 0.003))
prp(mod)
# tree pretty complex... too small to see decision variables
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.8332145

mod <- rpart(motor_UPDRS ~ ., data = predVars, control = rpart.control(cp = 0.004))
prp(mod)
# tree pretty complex... too small to see decision variables
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.8301855

# BEST
mod <- rpart(motor_UPDRS ~ ., data = predVars, control = rpart.control(cp = 0.005))
prp(mod)
# tree pretty complex... too small to see decision variables
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.8132654

mod <- rpart(motor_UPDRS ~ ., data = predVars, control = rpart.control(cp = 0.006))
prp(mod)
# tree pretty complex... too small to see decision variables
predictions <- predict(mod)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.7906328
```


## Create Averaged Regression Tree Model (Forest Model)

```{r independent_forests}
N = 100 #number of trees

# create random forest (set of CART models)
modForest <- list(NULL)
for(i in 1:N){
  modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num))
}
# predict outcomes
forestVotes <- predict(modForest[[1]], newdata = train)
for(i in 2:N){
  forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
}
# matrix to dataframe
row.names(forestVotes) <- NULL
forestVotes <- as.data.frame(forestVotes)
# average "votes" to get predictions
predictions <- summarise_all(forestVotes, median)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1607421, 0.1569906, 0.1598161, 0.1428644, 0.1337121, 0.1378096, 0.1534035, 0.1564861


# with minbucket = 2
# create random forest (set of CART models)
modForest <- list(NULL)
for(i in 1:N){
  modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(minbucket = 2))
}
# predict outcomes
forestVotes <- predict(modForest[[1]], newdata = train)
for(i in 2:N){
  forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
}
row.names(forestVotes) <- NULL
forestVotes <- as.data.frame(forestVotes)
predictions <- summarise_all(forestVotes, median)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.2763034, 0.2696555, 0.2481399, 0.2161291, 0.2378518


# use k-fold cross-validation to determine appropriate cp
# define cp values to test
cpGrid <- seq(0.001, 0.03, 0.001)
# test cp values
R2 <- NULL # initiate Rsquared matrix
for(j in 1:length(cpGrid)){
  modForest <- list(NULL)
  for(i in 1:N){
    modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = cpGrid[j]))
  }
  # predict outcomes
  forestVotes <- predict(modForest[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
  }
  # matrix to dataframe
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  # average "votes" to get predictions
  predictions <- summarise_all(forestVotes, median)
  # calculate Rsquared
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R2[j] = 1 - SSE/SST
}
# this loop takes almost a full minute to run
plot(cpGrid, R2)
# suggested cp = 0.002, 0.026, 0.011, 0.029, 0.019, 0.026
# the plots do not show the expected smooth curve (possibly paritally as a result of the random selections of data), so no clear place where a curve is at max

# test wider set of cp
cpGrid <- seq(0.01, 0.5, 0.01)
# test cp values
R2 <- NULL # initiate Rsquared matrix
for(j in 1:length(cpGrid)){
  modForest <- list(NULL)
  for(i in 1:N){
    modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = cpGrid[j]))
  }
  # predict outcomes
  forestVotes <- predict(modForest[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
  }
  # matrix to dataframe
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  # average "votes" to get predictions
  predictions <- summarise_all(forestVotes, median)
  # calculate Rsquared
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R2[j] = 1 - SSE/SST
}
plot(cpGrid, R2)
# these plots are more smooth and of the expected shape after about 0.15, but before that, the points don't show a definite trend (overall, a decrease in accuracy with increasing cp, but the points are fairly spread out around this central trend)
# one solution may be adding more trees. Especially since each tree is only built based on 38 observations, a large number of trees should help increase the reliability of the forest


# test more trees
N = 200
# use k-fold cross-validation to determine appropriate cp
# define cp values to test, start wide
cpGrid <- seq(0.01, 0.5, 0.01)
# test cp values
R2 <- NULL # initiate Rsquared matrix
for(j in 1:length(cpGrid)){
  modForest <- list(NULL)
  for(i in 1:N){
    modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = cpGrid[j]))
  }
  # predict outcomes
  forestVotes <- predict(modForest[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
  }
  # matrix to dataframe
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  # average "votes" to get predictions
  predictions <- summarise_all(forestVotes, median)
  # calculate Rsquared
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R2[j] = 1 - SSE/SST
}
# takes a few minutes...
plot(cpGrid, R2)

# test even more trees
N = 400
# use k-fold cross-validation to determine appropriate cp
# define cp values to test, start wide
cpGrid <- seq(0.01, 0.5, 0.01)
# test cp values
R2 <- NULL # initiate Rsquared matrix
for(j in 1:length(cpGrid)){
  modForest <- list(NULL)
  for(i in 1:N){
    modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = cpGrid[j]))
  }
  # predict outcomes
  forestVotes <- predict(modForest[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
  }
  # matrix to dataframe
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  # average "votes" to get predictions
  predictions <- summarise_all(forestVotes, median)
  # calculate Rsquared
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R2[j] = 1 - SSE/SST
}
# takes a WHILE, good thing we only have to run this for tuning D:
plot(cpGrid, R2)
# definitely looking cleaner, lets try one more...

# test EVEN MORE trees!
N = 600
# use k-fold cross-validation to determine appropriate cp
# define cp values to test, start wide
cpGrid <- seq(0.01, 0.5, 0.01)
# test cp values
R2 <- NULL # initiate Rsquared matrix
for(j in 1:length(cpGrid)){
  modForest <- list(NULL)
  for(i in 1:N){
    modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = cpGrid[j]))
  }
  # predict outcomes
  forestVotes <- predict(modForest[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
  }
  # matrix to dataframe
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  # average "votes" to get predictions
  predictions <- summarise_all(forestVotes, median)
  # calculate Rsquared
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R2[j] = 1 - SSE/SST
}
# takes about 10 minutes to run loop
plot(cpGrid, R2)
# not significantly cleaner than N=400, lets go with N=400 for now


# test more specific cp range
N = 400
# use k-fold cross-validation to determine appropriate cp
# define cp values to test, start wide
cpGrid <- seq(0.01, 0.2, 0.01)
# test cp values
R2 <- NULL # initiate Rsquared matrix
for(j in 1:length(cpGrid)){
  modForest <- list(NULL)
  for(i in 1:N){
    modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = cpGrid[j]))
  }
  # predict outcomes
  forestVotes <- predict(modForest[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
  }
  # matrix to dataframe
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  # average "votes" to get predictions
  predictions <- summarise_all(forestVotes, median)
  # calculate Rsquared
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R2[j] = 1 - SSE/SST
}
# about 3 min
plot(cpGrid, R2)
# cp = 0.1, 0.07 (but 0.1 was up there too...), 0.07 (looks like high outlier, but also near the cusp point of trend...), 0.06 (looks like high outlier, cusp closer to 0.1), 0.1 (though 0.08 and 0.12 look almost as good, 0.1 looks like it's on the cusp)
# lets go with 0.1



# BEST MODEL
# set parameters
N = 400
CP = 0.1
# create algorithm
modForest <- list(NULL)
for(i in 1:N){
  modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = CP))
}
# predict outcomes
forestVotes <- predict(modForest[[1]], newdata = train)
for(i in 2:N){
  forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
}
row.names(forestVotes) <- NULL
forestVotes <- as.data.frame(forestVotes)
predictions <- summarise_all(forestVotes, median)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1700616, 0.1698347, 0.1773354, 0.1603198, 0.1787604, 0.1886753, 0.1462228, 0.163786, 0.1779172, 0.162883
# 0.1695796 +/- 0.01196566

# set parameters
N = 500
CP = 0.1
# create algorithm
modForest <- list(NULL)
for(i in 1:N){
  modForest[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = CP))
}
# predict outcomes
forestVotes <- predict(modForest[[1]], newdata = train)
for(i in 2:N){
  forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
}
row.names(forestVotes) <- NULL
forestVotes <- as.data.frame(forestVotes)
predictions <- summarise_all(forestVotes, median)
SSE = sum((train$motor_UPDRS - predictions)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1666291, 0.1656189, 0.1506248, 0.1754871, 0.1748978, 0.1661218, 0.1649591, 0.1649591, 0.1670423, 0.1693327
# 0.1665673 +/- 0.006809069
```

## Average More Linear Regression Models
To be consistent with the number of trees here, we should perhaps use the average of 400 models for linear regression on independent sets.

```{r N400_linreg}
N = 400

# Best model
model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
modVars <- select(train, motor_UPDRS, age, loud_PPE_median, sex, normal_DFA_median, time_of_day)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = modFunction[1, 1] + age * modFunction[1, 2] + loud_PPE_median * modFunction[1, 3] + sex * modFunction[1, 4] + normal_DFA_median * modFunction[1, 5] + time_of_day * modFunction[1, 6],  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1607778, 0.1605803, 0.1605396, 0.1607853, 0.1610193, 0.1608085, 0.1601694, 0.1559542, 0.1612683, 0.1548684
# 0.1596771 +/- 0.002281295


# Interactions model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction[1, 1]) + (age * modFunction[1, 2]) + (sex * modFunction[1, 3]) + (time_of_day * modFunction[1, 4]) + (loud_PPE_median * modFunction[1, 5]) + (normal_DFA_median * modFunction[1, 6]) + (age * sex * modFunction[1, 7]) + (age* time_of_day * modFunction[1, 8]) + (sex * time_of_day * modFunction[1, 9]) + (loud_PPE_median * normal_DFA_median * modFunction[1, 10]) + (age * sex * time_of_day * modFunction[1, 11]),  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.223999, 0.2013596, 0.240382, 0.1146053, 0.1670987, 0.2508134, 0.2346001, 0.152405, 0.08659697, 0.1878007
# 0.1859661 +/- 0.05544494


# Other Interactions Model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction[1, 1]) + (age * modFunction[1, 2]) + (sex * modFunction[1, 3]) + (time_of_day * modFunction[1, 4]) + (loud_PPE_median * modFunction[1, 5]) + (normal_DFA_median * modFunction[1, 6]) + (age * sex * modFunction[1, 7]) + (age* time_of_day * modFunction[1, 8]) + (sex * time_of_day * modFunction[1, 9]) + (age * sex * time_of_day * modFunction[1, 10]),  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.2385361, 0.1889375, 0.1804549, 0.1792446, 0.09349722, 0.1559137, 0.2280434, 0.2044737, 0.1310816, 0.2285579
# 0.1828741 +/- 0.04612232

```

```{r N500_linreg}
N = 500

# Best model
model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
modVars <- select(train, motor_UPDRS, age, loud_PPE_median, sex, normal_DFA_median, time_of_day)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = modFunction[1, 1] + age * modFunction[1, 2] + loud_PPE_median * modFunction[1, 3] + sex * modFunction[1, 4] + normal_DFA_median * modFunction[1, 5] + time_of_day * modFunction[1, 6],  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1580983, 0.1613743, 0.1612573, 0.156041, 0.1615942, 0.1611177, 0.1617595, 0.1607926, 0.1615106, 0.1608913
# w/500: 0.1604437 +/- 0.001867328
# vs. w/100: 0.1552234 +/- 0.007381991
# vs. w/400: 0.1596771 +/- 0.002281295


# Interactions model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction[1, 1]) + (age * modFunction[1, 2]) + (sex * modFunction[1, 3]) + (time_of_day * modFunction[1, 4]) + (loud_PPE_median * modFunction[1, 5]) + (normal_DFA_median * modFunction[1, 6]) + (age * sex * modFunction[1, 7]) + (age* time_of_day * modFunction[1, 8]) + (sex * time_of_day * modFunction[1, 9]) + (loud_PPE_median * normal_DFA_median * modFunction[1, 10]) + (age * sex * time_of_day * modFunction[1, 11]),  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.2539031, 0.2318817, 0.2264244, 0.1807191, 0.2579103, 0.2184624, 0.2299395, 0.2465508, 0.2136945, 0.2527654
# w/500: 0.2312251 +/- 0.02350283
# vs. w/100: -0.1031874 +/- 0.5120816
# vs. w/400: 0.1859661 +/- 0.05544494

# Other Interactions Model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction[1, 1]) + (age * modFunction[1, 2]) + (sex * modFunction[1, 3]) + (time_of_day * modFunction[1, 4]) + (loud_PPE_median * modFunction[1, 5]) + (normal_DFA_median * modFunction[1, 6]) + (age * sex * modFunction[1, 7]) + (age* time_of_day * modFunction[1, 8]) + (sex * time_of_day * modFunction[1, 9]) + (age * sex * time_of_day * modFunction[1, 10]),  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.2051485, 0.1991503, 0.1625494, 0.2349779, 0.1528565, 0.2295555, 0.0784994, 0.2356489, 0.2368504, 0.2358559
# w/500: 0.1971093 +/- 0.05200699
# vs. w/100: 0.09214773 +/- 0.1527192
# vs. w/400: 0.1828741 +/- 0.04612232
```


# Out-of-Sample Performance
The days with no loud recordings ended up in the test set. These days cause issues when using the models to predict. Too many values are missing from these observations to use MICE to impute them. We will remove these observations.

```{r remove_missing}
test <- subset(test, loud_recs_count >= 1)

```

Test models' predictive power on the test set (new data).

```{r out-of-sample_tests}
# Standard Linreg, Best Variables
# build model
model1 <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = train)
# predict test set
predictTest = predict(model1, newdata = test)
# out-of-sample R2
SSE = sum((test$motor_UPDRS - predictTest)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.04849333


# Standard Linreg, More Interactions
# build model
model2 <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = train)
# predict test set
predictTest = predict(model2, newdata = test)
# out-of-sample R2
SSE = sum((test$motor_UPDRS - predictTest)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1001847


# Standard Linreg, Less Interactions
# build model
model3 <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = train)
# predict test set
predictTest = predict(model3, newdata = test)
# out-of-sample R2
SSE = sum((test$motor_UPDRS - predictTest)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.089698


# Standard CART, All Variables
# build model
model4 <- rpart(motor_UPDRS ~ ., data = predVars, control = rpart.control(cp = 0.005))
# predict test set
predictTest = predict(model4, newdata = test)
# out-of-sample R2
SSE = sum((test$motor_UPDRS - predictTest)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.2498027



N = 1500

# AvgdIndep Linreg, Best Variables
# build model
model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction5 <- summarise_all(modCoeffs, median)
# predict test set
modVars <- select(test, motor_UPDRS, age, loud_PPE_median, sex, normal_DFA_median, time_of_day)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction5[1, 1]) + 
                    (age * modFunction5[1, 2]) + 
                    (loud_PPE_median * modFunction5[1, 3]) + 
                    (sex * modFunction5[1, 4]) + 
                    (normal_DFA_median * modFunction5[1, 5]) + 
                    (time_of_day * modFunction5[1, 6]),  
                  residuals = motor_UPDRS - prediction)
# out-of-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.06327727, 0.0504753, 0.05388428, 0.05446356, 0.05504083, 0.05673081, 0.06135233, 0.03668167, 0.05219317, 0.04576175
# 0.0529861 +/- 0.007616465


# AvgdIndep Linreg, More Interactions
# build model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction6 <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(test, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction6[1, 1]) + 
                    (age * modFunction6[1, 2]) + 
                    (sex * modFunction6[1, 3]) + 
                    (time_of_day * modFunction6[1, 4]) + 
                    (loud_PPE_median * modFunction6[1, 5]) + 
                    (normal_DFA_median * modFunction6[1, 6]) + 
                    (age * sex * modFunction6[1, 7]) + 
                    (age* time_of_day * modFunction6[1, 8]) + 
                    (sex * time_of_day * modFunction6[1, 9]) + 
                    (loud_PPE_median * normal_DFA_median * modFunction6[1, 10]) + 
                    (age * sex * time_of_day * modFunction6[1, 11]),  
                  residuals = motor_UPDRS - prediction)
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.08789277, 0.100658, -0.03565, 0.07232206, -0.04839899, 0.06165373, -0.003125171, 0.0748603, -0.04179203, -0.09300411
# 0.01754166 +/- 0.06947045


# AvgdIndep Linreg, Less Interactions
# build model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- NULL
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction7 <- summarise_all(modCoeffs, median)
# predict values
modVars <- select(test, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction7[1, 1]) + 
                    (age * modFunction7[1, 2]) + 
                    (sex * modFunction7[1, 3]) + 
                    (time_of_day * modFunction7[1, 4]) + 
                    (loud_PPE_median * modFunction7[1, 5]) + 
                    (normal_DFA_median * modFunction7[1, 6]) + 
                    (age * sex * modFunction7[1, 7]) + 
                    (age* time_of_day * modFunction7[1, 8]) + 
                    (sex * time_of_day * modFunction7[1, 9]) + 
                    (age * sex * time_of_day * modFunction7[1, 10]),  
                  residuals = motor_UPDRS - prediction)
# out-of-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.07697862, 0.06753241, 0.07255408, 0.091085, 0.1041976, 0.1024762, 0.0679015, 0.09112426, 0.0928736, 0.007609233
# 0.07743325 +/- 0.02797615



# AvgdIndep CART, All Variables
# create algorithm
modForest8 <- list(NULL)
for(i in 1:N){
  modForest8[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = 0.1))
}
# predict outcomes
forestVotes <- predict(modForest8[[1]], newdata = test)
for(i in 2:N){
  forestVotes <- rbind(forestVotes, predict(modForest8[[i]], newdata = test))
}
row.names(forestVotes) <- NULL
forestVotes <- as.data.frame(forestVotes)
predictions <- summarise_all(forestVotes, median)
# out-of-sample R2
SSE = sum((test$motor_UPDRS - predictions)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.0766452, 0.06680034, 0.08125375, 0.07336874, 0.05608311, 0.08585367, 0.04868608, 0.06694463, 0.05566609, 0.04634199
# 0.06576436 +/- 0.01369016
# 0.0766096

```

# Increase N

Increase N to reduce the variability in models' predictive power when model is built multiple times 

```{r}
predVars <- select(train, -subject_num, -test_day, -contains("count"), -total_UPDRS)

test <- subset(test, loud_recs_count >= 1)
```


```{r optimize_N}
N = 1500


# AvgdIndep Linreg, Best Variables
R_Linreg5 <- NULL
for(R in 1:10){
  # build model
  model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
  modCoeffs <- model$coefficients
  for(i in 2:N){
    model <- NULL
    model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = independent_selector(train, train$subject_num))
    modCoeffs <- rbind(modCoeffs, model$coefficients)
  }
  row.names(modCoeffs) <- NULL
  modCoeffs <- as.data.frame(modCoeffs)
  modFunction5 <- summarise_all(modCoeffs, median)
  # predict train set
  modVars <- select(train, motor_UPDRS, age, loud_PPE_median, sex, normal_DFA_median, time_of_day)
  modVars$sex <- as.numeric(modVars$sex) - 1
  modVars <- mutate(modVars, prediction = (modFunction5[1, 1]) + 
                      (age * modFunction5[1, 2]) + 
                      (loud_PPE_median * modFunction5[1, 3]) + 
                      (sex * modFunction5[1, 4]) + 
                      (normal_DFA_median * modFunction5[1, 5]) + 
                      (time_of_day * modFunction5[1, 6]),  
                    residuals = motor_UPDRS - prediction)
  # in-sample R2
  SSE = sum(modVars$residuals^2)
  SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
  R_Linreg5[R] = 1 - SSE/SST
}


# AvgdIndep Linreg, More Interactions
R_Linreg6 <- NULL
for(R in 1:10){
  # build model
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- model$coefficients
  for(i in 2:N){
    model <- NULL
    model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = independent_selector(train, train$subject_num))
    modCoeffs <- rbind(modCoeffs, model$coefficients)
  }
  row.names(modCoeffs) <- NULL
  modCoeffs <- as.data.frame(modCoeffs)
  modFunction6 <- summarise_all(modCoeffs, median)
  # predict train set
  modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
  modVars$sex <- as.numeric(modVars$sex) - 1
  modVars <- mutate(modVars, prediction = (modFunction6[1, 1]) + 
                      (age * modFunction6[1, 2]) + 
                      (sex * modFunction6[1, 3]) + 
                      (time_of_day * modFunction6[1, 4]) + 
                      (loud_PPE_median * modFunction6[1, 5]) + 
                      (normal_DFA_median * modFunction6[1, 6]) + 
                      (age * sex * modFunction6[1, 7]) + 
                      (age* time_of_day * modFunction6[1, 8]) + 
                      (sex * time_of_day * modFunction6[1, 9]) + 
                      (loud_PPE_median * normal_DFA_median * modFunction6[1, 10]) + 
                      (age * sex * time_of_day * modFunction6[1, 11]),  
                    residuals = motor_UPDRS - prediction)
  # in-sample R2
  SSE = sum(modVars$residuals^2)
  SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
  R_Linreg6[R] = 1 - SSE/SST
}


# AvgdIndep Linreg, Less Interactions
R_Linreg7 <- NULL
for(R in 1:10){
  # build model
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
  modCoeffs <- model$coefficients
  for(i in 2:N){
    model <- NULL
    model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = independent_selector(train, train$subject_num))
    modCoeffs <- rbind(modCoeffs, model$coefficients)
  }
  row.names(modCoeffs) <- NULL
  modCoeffs <- as.data.frame(modCoeffs)
  modFunction7 <- summarise_all(modCoeffs, median)
  # predict train set
  modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
  modVars$sex <- as.numeric(modVars$sex) - 1
  modVars <- mutate(modVars, prediction = (modFunction7[1, 1]) + 
                      (age * modFunction7[1, 2]) + 
                      (sex * modFunction7[1, 3]) + 
                      (time_of_day * modFunction7[1, 4]) + 
                      (loud_PPE_median * modFunction7[1, 5]) + 
                      (normal_DFA_median * modFunction7[1, 6]) + 
                      (age * sex * modFunction7[1, 7]) + 
                      (age* time_of_day * modFunction7[1, 8]) + 
                      (sex * time_of_day * modFunction7[1, 9]) + 
                      (age * sex * time_of_day * modFunction7[1, 10]),  
                    residuals = motor_UPDRS - prediction)
  # in-sample R2
  SSE = sum(modVars$residuals^2)
  SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
  R_Linreg7[R] = 1 - SSE/SST
}


# AvgdIndep CART, All Variables
R_CART8 <- NULL
for(R in 1:10){
  # build model
  modForest8 <- list(NULL)
  for(i in 1:N){
    modForest8[[i]] <- rpart(motor_UPDRS ~ ., data = independent_selector(predVars, train$subject_num), control = rpart.control(cp = 0.1))
  }
  # predict outcomes
  forestVotes <- predict(modForest8[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest8[[i]], newdata = train))
  }
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  predictions <- summarise_all(forestVotes, median)
  # in-sample R2
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R_CART8[R] = 1 - SSE/SST
}


performance <- c(N, mean(R_Linreg5), sd(R_Linreg5), mean(R_Linreg6), sd(R_Linreg6), mean(R_Linreg7), sd(R_Linreg7), mean(R_CART8), sd(R_CART8))

#optimumN <- performance

optimumN <- rbind(optimumN, performance)


# rownames(optimumN) <- NULL
# optimumN <- as.data.frame(optimumN)
# names(optimumN) <- c("N", "R_Linreg5_mean", "R_Linreg5_sd", "R_Linreg6_mean", "R_Linreg6_sd", "R_Linreg7_mean", "R_Linreg7_sd", "R_CART8_mean", "R_CART8_sd")
```


## Build One Final Model for Each Model Type
### Seed Setting
```{r testing_seed_setting}
N = 10

set.seed(500)

independent_train1 <- list(NULL)
for(i in 1:N){
  independent_train1[[i]] <- independent_selector(train, train$subject_num)
}

independent_train2 <- list(NULL)
for(i in 1:N){
  independent_train2[[i]] <- independent_selector(train, train$subject_num)
}

sum(independent_train1[[10]]$test_day != independent_train2[[10]]$test_day)
# each set in 1 is equal to parallel set in 2

sum(independent_train1[[1]]$test_day == independent_train1[[10]]$test_day)
# none of the sets within 1 are equal to each other

```

Bottom line: seed setting works

### Set-up
```{r data}
# select training set for averaged models
N = 2000
set.seed(500)

indep_train <- list(NULL)
for(i in 1:N){
  indep_train[[i]] <- independent_selector(train, train$subject_num)
}


# CART variables
predVars <- select(train, -subject_num, -test_day, -contains("count"), -total_UPDRS)

indep_predVars <- list(NULL)
for(i in 1:N){
  indep_predVars[[i]] <- select(indep_train[[i]], -subject_num, -test_day, -contains("count"), -total_UPDRS)
}


# remove observations with too much missing data to predict
test <- subset(test, loud_recs_count >= 1)

```
runtime: ~6s

Since some of the parameters around averaged CART model have changed (N, seed set for training data), want to check that this cp is still what I would have chosen.
```{r check_cp}

N = 2000

# define cp values to test
cpGrid <- seq(0.01, 0.5, 0.01)

# test cp values
R2 <- NULL # initiate Rsquared matrix
for(j in 1:length(cpGrid)){
  modForest <- list(NULL)
  for(i in 1:N){
    modForest[[i]] <- rpart(motor_UPDRS ~ ., data = indep_predVars[[i]], control = rpart.control(cp = cpGrid[j]))
  }
  # predict outcomes
  forestVotes <- predict(modForest[[1]], newdata = train)
  for(i in 2:N){
    forestVotes <- rbind(forestVotes, predict(modForest[[i]], newdata = train))
  }
  # matrix to dataframe
  row.names(forestVotes) <- NULL
  forestVotes <- as.data.frame(forestVotes)
  # average "votes" to get predictions
  predictions <- summarise_all(forestVotes, median)
  # calculate Rsquared
  SSE = sum((train$motor_UPDRS - predictions)^2)
  SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
  R2[j] = 1 - SSE/SST
}
# LONG runtime (saved plot) - 3:33
plot(cpGrid, R2)
# Still looks like 0.1

```
Confirmed: use cp = 0.1 for averaged CART


### Build Models & Predict Train Set
```{r build_models}
# initiate list for storing models
Models <- list(NULL)

# initiate dataframe for predictions
plotTrainData <- select(train, subject_num, test_day, motor_UPDRS)



# Standard Linreg, Best Variables
# build model
model1 <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = train)
# save model
Models[[1]] <- model1$coefficients
# in-sample R2 = 0.1622
# train set accuracy
predictions <- predict(model1, newdata = train)
residuals <- train$motor_UPDRS - predictions
sum(abs(residuals) <= 5.2)/length(residuals) # 0.5048951
sum(abs(residuals) <= 2.5)/length(residuals) # 0.2237762
# save predictions
plotTrainData$mod1_predict <- predictions


# Standard Linreg, Less Interactions
# build model
model2 <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = train)
# save model
Models[[2]] <- model2$coefficients
# in-sample R2 = 0.2444
# train set accuracy
predictions <- predict(model2, newdata = train)
residuals <- train$motor_UPDRS - predictions
sum(abs(residuals) <= 5.2)/length(residuals) # 0.565035
sum(abs(residuals) <= 2.5)/length(residuals) # 0.2713287
# save predictions
plotTrainData$mod2_predict <- predictions


# Standard Linreg, More Interactions
# build model
model3 <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = train)
# save model
Models[[3]] <- model3$coefficients
# in-sample R2 = 0.2653
# train set accuracy
predictions <- predict(model3, newdata = train)
residuals <- train$motor_UPDRS - predictions
sum(abs(residuals) <= 5.2)/length(residuals) # 0.579021
sum(abs(residuals) <= 2.5)/length(residuals) # 0.3034965
# save predictions
plotTrainData$mod3_predict <- predictions

# Standard CART, All Variables
# build model
model4 <- rpart(motor_UPDRS ~ ., data = predVars, control = rpart.control(cp = 0.005))
# save model
Models[[4]] <- model4
# plot tree
prp(model4, varlen = 7)
# predict train set
predictions <- predict(model4)
residuals <- train$motor_UPDRS - predictions
# in-sample R2
SSE = sum((residuals)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2
# in-sample R2 = 0.8132654
# train set accuracy
sum(abs(residuals) <= 5.2)/length(residuals) # 0.8895105
sum(abs(residuals) <= 2.5)/length(residuals) # 0.6503497
# save predictions
plotTrainData$mod4_predict <- predictions



N = 2000

# AvgdIndep Linreg, Best Variables
# build model
model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = indep_train[[1]])
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- lm(motor_UPDRS ~ age + loud_PPE_median + sex + normal_DFA_median + time_of_day, data = indep_train[[i]])
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction5 <- summarise_all(modCoeffs, median)
# save model
Models[[5]] <- modFunction5
# predict train set
modVars <- select(train, motor_UPDRS, age, loud_PPE_median, sex, normal_DFA_median, time_of_day)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction5[1, 1]) + 
                    (age * modFunction5[1, 2]) + 
                    (loud_PPE_median * modFunction5[1, 3]) + 
                    (sex * modFunction5[1, 4]) + 
                    (normal_DFA_median * modFunction5[1, 5]) + 
                    (time_of_day * modFunction5[1, 6]),  
                  residuals = motor_UPDRS - prediction)
# in-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2
# in-sample R2 = 0.1611757
# train set accuracy
sum(abs(modVars$residuals) <= 5.2)/length(modVars$residuals) # 0.5118881
sum(abs(modVars$residuals) <= 2.5)/length(modVars$residuals) # 0.2265734
# save predictions
plotTrainData$mod5_predict <- modVars$prediction


# AvgdIndep Linreg, Less Interactions
# build model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = indep_train[[1]])
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median + normal_DFA_median, data = indep_train[[i]])
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction6 <- summarise_all(modCoeffs, median)
# save model
Models[[6]] <- modFunction6
# predict train set
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction6[1, 1]) + 
                    (age * modFunction6[1, 2]) + 
                    (sex * modFunction6[1, 3]) + 
                    (time_of_day * modFunction6[1, 4]) + 
                    (loud_PPE_median * modFunction6[1, 5]) + 
                    (normal_DFA_median * modFunction6[1, 6]) + 
                    (age * sex * modFunction6[1, 7]) + 
                    (age* time_of_day * modFunction6[1, 8]) + 
                    (sex * time_of_day * modFunction6[1, 9]) + 
                    (age * sex * time_of_day * modFunction6[1, 10]),  
                  residuals = motor_UPDRS - prediction)
# in-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 
# in-sample R2 = 0.2233552
# train set accuracy
sum(abs(modVars$residuals) <= 5.2)/length(modVars$residuals) # 0.5608392
sum(abs(modVars$residuals) <= 2.5)/length(modVars$residuals) # 0.2265734
# save predictions
plotTrainData$mod6_predict <- modVars$prediction


# AvgdIndep Linreg, More Interactions
# build model
model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = indep_train[[1]])
modCoeffs <- model$coefficients
for(i in 2:N){
  model <- lm(motor_UPDRS ~ age*sex*time_of_day + loud_PPE_median*normal_DFA_median, data = indep_train[[i]])
  modCoeffs <- rbind(modCoeffs, model$coefficients)
}
row.names(modCoeffs) <- NULL
modCoeffs <- as.data.frame(modCoeffs)
modFunction7 <- summarise_all(modCoeffs, median)
# save model
Models[[7]] <- modFunction7
# predict train set
modVars <- select(train, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction7[1, 1]) + 
                    (age * modFunction7[1, 2]) + 
                    (sex * modFunction7[1, 3]) + 
                    (time_of_day * modFunction7[1, 4]) + 
                    (loud_PPE_median * modFunction7[1, 5]) + 
                    (normal_DFA_median * modFunction7[1, 6]) + 
                    (age * sex * modFunction7[1, 7]) + 
                    (age* time_of_day * modFunction7[1, 8]) + 
                    (sex * time_of_day * modFunction7[1, 9]) + 
                    (loud_PPE_median * normal_DFA_median * modFunction7[1, 10]) + 
                    (age * sex * time_of_day * modFunction7[1, 11]),  
                  residuals = motor_UPDRS - prediction)
# in-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2
# in-sample R2 = 0.260705
# train set accuracy
sum(abs(modVars$residuals) <= 5.2)/length(modVars$residuals) # 0.5692308
sum(abs(modVars$residuals) <= 2.5)/length(modVars$residuals) # 0.3048951
# save predictions
plotTrainData$mod7_predict <- modVars$prediction


# AvgdIndep CART, All Variables
# build model
modForest8 <- list(NULL)
for(i in 1:N){
  modForest8[[i]] <- rpart(motor_UPDRS ~ ., data = indep_predVars[[i]], control = rpart.control(cp = 0.1))
}
# save model
Models[[8]] <- modForest8
# predict train set
forestVotes <- predict(modForest8[[1]], newdata = train)
for(i in 2:N){
  forestVotes <- rbind(forestVotes, predict(modForest8[[i]], newdata = train))
}
row.names(forestVotes) <- NULL
forestVotes <- as.data.frame(forestVotes)
predictions <- summarise_all(forestVotes, median)
residuals <- train$motor_UPDRS - predictions
# in-sample R2
SSE = sum((residuals)^2)
SST = sum((train$motor_UPDRS - mean(train$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2
# in-sample R2 = 0.172459
# train set accuracy
sum(abs(residuals) <= 5.2)/length(residuals) # 0.4937063
sum(abs(residuals) <= 2.5)/length(residuals) # 0.2307692
# save predictions
plotTrainData$mod8_predict <- unname(unlist(predictions[1,]))
```
RUNTIMES

modFunction5
build: ~4s
predict: ~1s

modFunction6
build: ~4s
predict: ~1s

modfunction7
build: ~4s
predict: ~1s

modForest8
build: ~26s
predict: ~50s
test predict: ~13s


### Predict Train Set
```{r test_models}
# initiate dataframe for predictions
plotTestData <- select(test, subject_num, test_day, motor_UPDRS)



# Standard Linreg, Best Variables
# predict test set
predictTest <- predict(model1, newdata = test)
residuals <- test$motor_UPDRS - predictTest
# out-of-sample R2
SSE = sum((residuals)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.04849333
# test set accuracy
sum(abs(residuals) <= 5.2)/length(residuals) # 0.3970588
sum(abs(residuals) <= 2.5)/length(residuals) # 0.1838235
# save predictions
plotTestData$mod1_predict <- predictTest


# Standard Linreg, Less Interactions
# predict test set
predictTest = predict(model2, newdata = test)
residuals <- test$motor_UPDRS - predictTest
# out-of-sample R2
SSE = sum((residuals)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.089698
# test set accuracy
sum(abs(residuals) <= 5.2)/length(residuals) # 0.4191176
sum(abs(residuals) <= 2.5)/length(residuals) # 0.2316176
# save predictions
plotTestData$mod2_predict <- predictTest


# Standard Linreg, More Interactions
# predict test set
predictTest = predict(model3, newdata = test)
residuals <- test$motor_UPDRS - predictTest
# out-of-sample R2
SSE = sum((residuals)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1001847
# test set accuracy
sum(abs(residuals) <= 5.2)/length(residuals) # 0.4301471
sum(abs(residuals) <= 2.5)/length(residuals) # 0.2389706
# save predictions
plotTestData$mod3_predict <- predictTest


# Standard CART, All Variables
# predict test set
predictTest = predict(model4, newdata = test)
residuals <- test$motor_UPDRS - predictTest
# out-of-sample R2
SSE = sum((residuals)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.2498027
# test set accuracy
sum(abs(residuals) <= 10.8)/length(residuals) # 0.8492647
sum(abs(residuals) <= 5.2)/length(residuals) # 0.5625
sum(abs(residuals) <= 2.5)/length(residuals) # 0.3860294
# save predictions
plotTestData$mod4_predict <- predictTest



N = 2000

# AvgdIndep Linreg, Best Variables
# predict test set
modVars <- select(test, motor_UPDRS, age, loud_PPE_median, sex, normal_DFA_median, time_of_day)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction5[1, 1]) + 
                    (age * modFunction5[1, 2]) + 
                    (loud_PPE_median * modFunction5[1, 3]) + 
                    (sex * modFunction5[1, 4]) + 
                    (normal_DFA_median * modFunction5[1, 5]) + 
                    (time_of_day * modFunction5[1, 6]),  
                  residuals = motor_UPDRS - prediction)
# out-of-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.04975923
# test set accuracy
sum(abs(modVars$residuals) <= 5.2)/length(modVars$residuals) # 0.4080882
sum(abs(modVars$residuals) <= 2.5)/length(modVars$residuals) # 0.1838235
# save predictions
plotTestData$mod5_predict <- modVars$prediction


# AvgdIndep Linreg, Less Interactions
# predict test set
modVars <- select(test, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction6[1, 1]) + 
                    (age * modFunction6[1, 2]) + 
                    (sex * modFunction6[1, 3]) + 
                    (time_of_day * modFunction6[1, 4]) + 
                    (loud_PPE_median * modFunction6[1, 5]) + 
                    (normal_DFA_median * modFunction6[1, 6]) + 
                    (age * sex * modFunction6[1, 7]) + 
                    (age* time_of_day * modFunction6[1, 8]) + 
                    (sex * time_of_day * modFunction6[1, 9]) + 
                    (age * sex * time_of_day * modFunction6[1, 10]),  
                  residuals = motor_UPDRS - prediction)
# out-of-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1030394
# test set accuracy
sum(abs(modVars$residuals) <= 5.2)/length(modVars$residuals) # 0.4301471
sum(abs(modVars$residuals) <= 2.5)/length(modVars$residuals) # 0.1985294
# save predictions
plotTestData$mod6_predict <- modVars$prediction


# AvgdIndep Linreg, More Interactions
# predict test set
modVars <- select(test, motor_UPDRS, age, sex, time_of_day, loud_PPE_median,  normal_DFA_median)
modVars$sex <- as.numeric(modVars$sex) - 1
modVars <- mutate(modVars, prediction = (modFunction7[1, 1]) + 
                    (age * modFunction7[1, 2]) + 
                    (sex * modFunction7[1, 3]) + 
                    (time_of_day * modFunction7[1, 4]) + 
                    (loud_PPE_median * modFunction7[1, 5]) + 
                    (normal_DFA_median * modFunction7[1, 6]) + 
                    (age * sex * modFunction7[1, 7]) + 
                    (age* time_of_day * modFunction7[1, 8]) + 
                    (sex * time_of_day * modFunction7[1, 9]) + 
                    (loud_PPE_median * normal_DFA_median * modFunction7[1, 10]) + 
                    (age * sex * time_of_day * modFunction7[1, 11]),  
                  residuals = motor_UPDRS - prediction)
# out-of-sample R2
SSE = sum(modVars$residuals^2)
SST = sum((modVars$motor_UPDRS - mean(modVars$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.1005509
# test set accuracy
sum(abs(modVars$residuals) <= 5.2)/length(modVars$residuals) # 0.4338235
sum(abs(modVars$residuals) <= 2.5)/length(modVars$residuals) # 0.2352941
# save predictions
plotTestData$mod7_predict <- modVars$prediction


# AvgdIndep CART, All Variables
# predict test set
forestVotes <- predict(modForest8[[1]], newdata = test)
for(i in 2:N){
  forestVotes <- rbind(forestVotes, predict(modForest8[[i]], newdata = test))
}
row.names(forestVotes) <- NULL
forestVotes <- as.data.frame(forestVotes)
predictions <- summarise_all(forestVotes, median)
residuals <- test$motor_UPDRS - predictions
# out-of-sample R2
SSE = sum((residuals)^2)
SST = sum((test$motor_UPDRS - mean(test$motor_UPDRS))^2)
R2 = 1 - SSE/SST
R2 # 0.06886496
# test set accuracy
sum(abs(residuals) <= 5.2)/length(residuals) # 0.3786765
sum(abs(residuals) <= 2.5)/length(residuals) # 0.1727941
# save predictions
plotTestData$mod8_predict <- unname(unlist(predictions[1,]))
```


## Accuracy

To get a more interpretable metric, we want to see how often the models could predict within a certain range.

According to a study of what constitutes a clinically important difference (CID) on motor UPDRS scores:
minimum CID: 2.5
moderate CID: 5.2
large CID: 10.8

Also of interest when determining this range - average change in motor UPDRS over the course of this 6 month study. Part of the point of the AHTD is to get more frequent measurements. If we can't get more accurate than just predicting the first or last UPDRS over the 6 months of the study, then the additional measurements may not be useful.
Mean change: ~5.2
Median change: ~4.5

We will determine the percent which fall into the range of [target UPDRS +/- 5.2], representing a deviation from the target with moderate clinical importance and also a difference comparable to the average change between 0 and 6 months visits. We may also calculate the percent which fall into the range of [target UPDRS +/- 2.5], which would represent a minimum deviation of clinical importance.

The code to calculate the percentages within these ranges will be added above, after the calculations of predictions and associated R2.


## Plot Outputs

```{r save_models_predictions}
saveRDS(Models, "Models.rds")
saveRDS(plotTrainData, "train_set_predictions.rds")
saveRDS(plotTestData, "test_set_predictions.rds")
```


```{r plot_predictions}
ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod1_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")

ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod2_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")

ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod3_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")

ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod4_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")

ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod5_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")

ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod6_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")

ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod7_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")

ggplot(plotTrainData) +
  geom_point(aes(x = motor_UPDRS, y = mod8_predict), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black")


# facet wrap
oneTrainPlot <- gather(plotTrainData, model, prediction, 4:11)

ggplot(oneTrainPlot) +
  geom_point(aes(x = motor_UPDRS, y = prediction), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black") +
  facet_wrap(~model, ncol = 4)

oneTestPlot <- gather(plotTestData, model, prediction, 4:11)

ggplot(oneTestPlot) +
  geom_point(aes(x = motor_UPDRS, y = prediction), color = "forest green") +
  geom_point(aes(x = motor_UPDRS, y = motor_UPDRS), color = "black") +
  facet_wrap(~model, ncol = 4)
```

